{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim \n",
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from Environment import *\n",
    "from experience_replay import *\n",
    "from brain import *\n",
    "from agent import *\n",
    "from Hyperparameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "duplicate-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "still-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\codes\\Deep-Reinforcement-Learning\\sliding_racket\\agent.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state).unsqueeze(0).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, episode: 28, last_reward: -1, epsilon: 0.7471720943315961, action: 0\n",
      "iteration: 1, episode: 28, last_reward: -1, epsilon: 0.5582661385478638, action: 0\n",
      "iteration: 2, episode: 28, last_reward: -1, epsilon: 0.41712087993322045, action: 2\n",
      "iteration: 3, episode: 28, last_reward: -1, epsilon: 0.3116610814491425, action: 2\n",
      "iteration: 4, episode: 28, last_reward: -1, epsilon: 0.232864462948006, action: 2\n",
      "iteration: 5, episode: 28, last_reward: -1, epsilon: 0.17398982847626407, action: 2\n",
      "iteration: 6, episode: 28, last_reward: -1, epsilon: 0.13000034453500542, action: 2\n",
      "iteration: 7, episode: 28, last_reward: -1, epsilon: 0.09713262969004913, action: 2\n",
      "iteration: 8, episode: 28, last_reward: -1, epsilon: 0.07257479035344938, action: 2\n",
      "iteration: 9, episode: 28, last_reward: -1, epsilon: 0.054225858104063294, action: 2\n",
      "iteration: 10, episode: 28, last_reward: -1, epsilon: 0.040516047966540916, action: 2\n",
      "iteration: 11, episode: 28, last_reward: -1, epsilon: 0.03027246041319977, action: 2\n",
      "iteration: 12, episode: 28, last_reward: -1, epsilon: 0.02261873764750081, action: 1\n",
      "iteration: 13, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 14, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 15, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 16, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 17, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 18, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 19, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 20, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 21, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 22, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 23, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 24, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 25, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 26, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 27, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 28, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 29, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 30, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 31, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 32, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 33, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 34, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 35, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 36, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 37, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 38, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 39, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 40, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 41, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 42, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 43, episode: 28, last_reward: -1, epsilon: 0.02, action: 1\n",
      "iteration: 44, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 45, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 46, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 47, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 48, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 49, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 50, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 51, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 52, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 53, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 54, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 55, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 56, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 57, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 58, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 59, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 60, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 61, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 62, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 63, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 64, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 65, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 66, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 67, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 68, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 69, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 70, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 71, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 72, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 73, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-36ecc9f98c43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mcurrent_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "rewards = collections.deque(maxlen=20)\n",
    "\n",
    "\n",
    "env = Environment(param.IMAGESIZE[0],param.IMAGESIZE[1])\n",
    "agent = Agent()\n",
    "# agent.loadKnowlage('best.dat')\n",
    "\n",
    "current_state = env.reset()\n",
    "\n",
    "for itr in range(param.ITER_NUM):\n",
    "    for eps in range(param.EPISODE_NUM):\n",
    "\n",
    "        action = agent.find_action(current_state)\n",
    "        new_state, reward, done= env.step(action)\n",
    "        agent.update_weights([current_state.clone(),action,reward,done,new_state])\n",
    "        current_state = new_state.clone()\n",
    "\n",
    "        if done:\n",
    "            print(f'iteration: {itr}, episode: {eps}, last_reward: {reward}, epsilon: {agent.epsilon}, action: {action}')\n",
    "            state = env.reset()\n",
    "            agent.reset()\n",
    "            break\n",
    "            \n",
    "    agent.saveKnowlage(\"best.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "rewards = collections.deque(maxlen=20)\n",
    "state_list = []\n",
    "\n",
    "\n",
    "env = Environment(30,30)\n",
    "jimi = Agent()\n",
    "jimi.loadKnowlage('best.dat')\n",
    "jimi.epsilon = 0\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for itr in range(50):\n",
    "    for eps in range(50):\n",
    "        image = state[0].cpu().detach().numpy().copy()\n",
    "        image = (cv2.resize(image,(300,300))*255).astype(np.uint8)\n",
    "        state_list.append(np.expand_dims(image,axis=2))\n",
    "        cv2.imshow('game',image)\n",
    "        cv2.waitKey(10)\n",
    "\n",
    "        action = jimi.find_action(state)\n",
    "        newState, reward, done= env.step(action)\n",
    "        state = newState.clone()\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            jimi.reset()\n",
    "            break\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "surrounded-guarantee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file test.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "clip = ImageSequenceClip(state_list, fps=20)\n",
    "clip.write_gif('test.gif', fps=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
