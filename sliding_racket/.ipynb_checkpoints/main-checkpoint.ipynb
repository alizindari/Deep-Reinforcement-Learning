{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "injured-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import stat\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim \n",
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import gym\n",
    "import gym.spaces\n",
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "from Environment import *\n",
    "from experience_replay import *\n",
    "from brain import *\n",
    "from agent import *\n",
    "from Hyperparameters import *\n",
    "# from ai import *\n",
    "# from gymWrappers import *\n",
    "# from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "still-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, episode: 28, last_reward: -1, epsilon: 0.7471720943315961, action: 2\n",
      "iteration: 1, episode: 28, last_reward: -1, epsilon: 0.5582661385478638, action: 0\n",
      "iteration: 2, episode: 28, last_reward: -1, epsilon: 0.41712087993322045, action: 2\n",
      "iteration: 3, episode: 28, last_reward: -1, epsilon: 0.3116610814491425, action: 2\n",
      "iteration: 4, episode: 28, last_reward: -1, epsilon: 0.232864462948006, action: 2\n",
      "iteration: 5, episode: 28, last_reward: -1, epsilon: 0.17398982847626407, action: 2\n",
      "iteration: 6, episode: 28, last_reward: -1, epsilon: 0.13000034453500542, action: 2\n",
      "iteration: 7, episode: 28, last_reward: -1, epsilon: 0.09713262969004913, action: 2\n",
      "iteration: 8, episode: 28, last_reward: -1, epsilon: 0.07257479035344938, action: 2\n",
      "iteration: 9, episode: 28, last_reward: -1, epsilon: 0.054225858104063294, action: 2\n",
      "iteration: 10, episode: 28, last_reward: 5, epsilon: 0.040516047966540916, action: 2\n",
      "iteration: 11, episode: 28, last_reward: 5, epsilon: 0.03027246041319977, action: 2\n",
      "iteration: 12, episode: 28, last_reward: 5, epsilon: 0.02261873764750081, action: 2\n",
      "iteration: 13, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 14, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 15, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 16, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 17, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 18, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 19, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 20, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 21, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 22, episode: 28, last_reward: -1, epsilon: 0.02, action: 1\n",
      "iteration: 23, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 24, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 25, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 26, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 27, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 28, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 29, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 30, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 31, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 32, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 33, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 34, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 35, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 36, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 37, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 38, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 39, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 40, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 41, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 42, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 43, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 44, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 45, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 46, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 47, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 48, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 49, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 50, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 51, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 52, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 53, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 54, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 55, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 56, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 57, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 58, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 59, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 60, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 61, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 62, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 63, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 64, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 65, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 66, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 67, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 68, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 69, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 70, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 71, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 72, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 73, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 74, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 75, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 76, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 77, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 78, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 79, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 80, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 81, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 82, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 83, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 84, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 85, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 86, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 87, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 88, episode: 28, last_reward: -1, epsilon: 0.02, action: 1\n",
      "iteration: 89, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 90, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 91, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 92, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 93, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 94, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 95, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 96, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 97, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 98, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 99, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 100, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 101, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 102, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 103, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 104, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 105, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 106, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 107, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 108, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 109, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 110, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 111, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 112, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 113, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 114, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 115, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 116, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 117, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 118, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 119, episode: 28, last_reward: 5, epsilon: 0.02, action: 0\n",
      "iteration: 120, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 121, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 122, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 123, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 124, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 125, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 126, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 127, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 128, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 129, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 130, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 131, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 132, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 133, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 134, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 135, episode: 28, last_reward: -1, epsilon: 0.02, action: 1\n",
      "iteration: 136, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 137, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 138, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 139, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 140, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 141, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 142, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 143, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 144, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 145, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 146, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 147, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 148, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 149, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 150, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 151, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 152, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 153, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 154, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 155, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 156, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 157, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 158, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 159, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 160, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 161, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 162, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 163, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 164, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 165, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 166, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 167, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 168, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 169, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 170, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 171, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 172, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 173, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 174, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 175, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 176, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 177, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 178, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 179, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 180, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 181, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 182, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 183, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 184, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 185, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 186, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 187, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 188, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 189, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 190, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 191, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 192, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 193, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 194, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 195, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 196, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 197, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 198, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 199, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 200, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 201, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 202, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 203, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 204, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 205, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 206, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 207, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 208, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 209, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 210, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 211, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 212, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 213, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 214, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 215, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 216, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 217, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 218, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 219, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 220, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 221, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 222, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 223, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 224, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 225, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 226, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 227, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 228, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 229, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 230, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 231, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 232, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 233, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 234, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 235, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 236, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 237, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 238, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 239, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 240, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 241, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 242, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 243, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 244, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 245, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 246, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 247, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 248, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 249, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 250, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 251, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 252, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 253, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 254, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 255, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 256, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 257, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 258, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 259, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 260, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 261, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 262, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 263, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 264, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 265, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 266, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 267, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 268, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 269, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 270, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 271, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 272, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 273, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 274, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 275, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 276, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 277, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 278, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 279, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 280, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 281, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 282, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 283, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 284, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 285, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 286, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 287, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 288, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 289, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 290, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 291, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 292, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 293, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 294, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 295, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 296, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 297, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 298, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 299, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 300, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 301, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 302, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 303, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 304, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 305, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 306, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 307, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 308, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 309, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 310, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 311, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 312, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 313, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 314, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 315, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 316, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 317, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 318, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 319, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 320, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 321, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 322, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 323, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 324, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 325, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 326, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 327, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 328, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 329, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 330, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 331, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 332, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 333, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 334, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 335, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 336, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 337, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 338, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 339, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 340, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 341, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 342, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 343, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KPS\\Desktop\\pong\\agent.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_states,next_states,rewards,actions,dones = [torch.tensor(element,dtype=torch.float32).to(self.device) for element in random_samples]\n",
      "C:\\Users\\KPS\\Desktop\\pong\\agent.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dones = torch.tensor(dones,dtype=torch.bool)\n",
      "C:\\Users\\KPS\\Desktop\\pong\\agent.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions,dtype=torch.int64)\n",
      "c:\\python64\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\python64\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\python64\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3367: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "c:\\python64\\lib\\site-packages\\numpy\\core\\_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "c:\\python64\\lib\\site-packages\\numpy\\core\\_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 344, episode: 28, last_reward: -1, epsilon: 0.02, action: 1\n",
      "iteration: 345, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 346, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 347, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 348, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 349, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 350, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 351, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 352, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 353, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 354, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 355, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 356, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 357, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 358, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 359, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 360, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 361, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 362, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 363, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 364, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 365, episode: 28, last_reward: 5, epsilon: 0.02, action: 0\n",
      "iteration: 366, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 367, episode: 28, last_reward: -1, epsilon: 0.02, action: 2\n",
      "iteration: 368, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 369, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n",
      "iteration: 370, episode: 28, last_reward: 5, epsilon: 0.02, action: 2\n",
      "iteration: 371, episode: 28, last_reward: -1, epsilon: 0.02, action: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-18211d0ed23f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mnewState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#         print(done)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mjimi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgiveFeedBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnewState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\pong\\agent.py\u001b[0m in \u001b[0;36mgiveFeedBack\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteraction_counter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mSYNC_TARGET_FRAMES\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_len\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mREPLAY_START_SIZE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python64\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python64\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python64\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python64\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "rewards = collections.deque(maxlen=20)\n",
    "\n",
    "\n",
    "param = Hyperparameters()\n",
    "env = Environment(30,30)\n",
    "jimi = Agent()\n",
    "# jimi.loadKnowlage('best.dat')\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "for itr in range(1000):\n",
    "    for eps in range(50):\n",
    "\n",
    "        action = jimi.find_action(state)\n",
    "        newState, reward, done= env.step(action)\n",
    "#         print(done)\n",
    "        jimi.giveFeedBack([state.clone(),action,reward,done,newState])\n",
    "        state = newState.clone()\n",
    "\n",
    "        if done:\n",
    "            print(f'iteration: {itr}, episode: {eps}, last_reward: {reward}, epsilon: {jimi.epsilon}, action: {action}')\n",
    "            state = env.reset()\n",
    "            jimi.reset()\n",
    "            break\n",
    "            \n",
    "    jimi.saveKnowlage(\"best.dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-explanation",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "x,y,z,s = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
